"""
Test regressions with fixed seed
expected files are in the "expected" folder
the filename has pattern pop_{n}_seed{seed}.json

Expected files are generated by running this script with regenerate = True. Once
run, you will need to manually copy all of the files within

    regression/report/test_regression_make_population/test_results/

to
    regression/expected/test_regression_make_population/

for the new baseline to take effect. You should expect to see folders by the name configs and figs within

    regression/expected/test_regression_make_population/
"""

import os
import re
import shutil
import fnmatch
import unittest
import tempfile
import numpy as np
import sciris as sc
import cv2
import filecmp
import difflib
import synthpops as sp
from synthpops import data_distributions as spdd
from synthpops import base as spb

try:
    from fpdf import FPDF
except Exception as E:
    print(f'Note: could not import fpdf, report not available ({E})')


# Whether to remove temporary files generated in the process
remove_files = True

# Whether to regenerate files
regenerate = False
# regenerate = True


class TestRegression(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cls.n = 20001
        cls.seed = 1001
        # change this to True if you need to re-generate the baseline
        cls.generateBaseline = regenerate
        cls.pdfDir = sc.thisdir(__file__, "regression", "report")
        cls.expectedDir = sc.thisdir(__file__, "regression", "expected")
        cls.datadir = sp.settings.datadir
        cls.do_save = True
        shutil.rmtree(cls.pdfDir, ignore_errors=True)
        os.makedirs(cls.pdfDir, exist_ok=True)
        # set params, make sure name is identical to param names
        cls.pars = sc.objdict(
            n=cls.n,
            rand_seed=cls.seed,
            location='seattle_metro',
            state_location='Washington',
            country_location='usa',
            max_contacts=None,
            with_industry_code=True,
            with_facilities=False,
            use_two_group_reduction=False,
            average_LTCF_degree=20,
            with_non_teaching_staff=True
        )
        cls.pop = sp.Pop(**cls.pars)

    def setUp(self):
        self.resultdir = tempfile.TemporaryDirectory().name
        self.figDir = os.path.join(self.resultdir, "figs")
        self.configDir = os.path.join(self.resultdir, "configs")
        os.makedirs(self.figDir, exist_ok=True)
        os.makedirs(self.configDir, exist_ok=True)

    def tearDown(self):
        shutil.copytree(self.resultdir, os.path.join(self.reportfolder, "test_results"),
                        ignore=shutil.ignore_patterns("pop*.json", "*pop.json"))
        if remove_files:
            shutil.rmtree(self.resultdir, ignore_errors=True)
        else:
            print(f'Results folder: {self.resultdir}')
            print('Automatic file removing is switched off;\nwhen you are done, please remove it manually')

    @classmethod
    def tearDownClass(cls):
        if cls.generateBaseline:
            print(f"Generated baseline files without comparison.\n please review at {os.path.join(cls.pdfDir, 'test_results')} and copy them to {cls.expectedDir}.")

    def test_regression_make_population(self):
        test_prefix = 'test_regression_make_population'
        self.reportfolder = os.path.join(self.pdfDir, test_prefix)
        os.makedirs(self.reportfolder, exist_ok=True)
        filename = os.path.join(self.resultdir, f'pop_{self.n}_seed{self.seed}.json')
        self.run_regression(filename, test_prefix, self.configDir)

    def test_summary(self):
        test_prefix = self._testMethodName
        self.reportfolder = os.path.join(self.pdfDir, test_prefix)
        os.makedirs(self.reportfolder, exist_ok=True)
        actual_file = os.path.join(self.resultdir, "summary.json")
        sc.savejson(actual_file, self.pop.summary)
        expected_file = os.path.join(self.expectedDir, test_prefix, "summary.json")
        if not os.path.exists(expected_file):
            raise ValueError(self.msg_baseline_not_found(test_prefix, "summary.json"))
        passed, failedcase = self.check_result(actual_folder=self.resultdir,
                                               expected_folder=os.path.join(self.expectedDir, test_prefix),
                                               test_prefix=test_prefix, decimal=3, filepattern="summary.json")
        if not passed:
            self.check_files_diff(expected_file, actual_file)

    @unittest.skip("This is just to show an example of adding a different scenario will work")
    def test_regression_lower_teacher_age(self):
        # set params, make sure name is identical to param names
        pars = sc.objdict(
        n = self.n,
        rand_seed = self.seed,
        max_contacts = None,
        teacher_age_min = 20,
        teacher_age_max = 65,
        staff_age_min = 18,
        staff_age_max = 60,
        with_non_teaching_staff = True)
        #
        test_prefix = 'test_regression_lower_teacher_age'
        self.reportfolder = os.path.join(self.pdfDir, test_prefix)
        os.makedirs(self.reportfolder, exist_ok=True)
        filename = os.path.join(self.resultdir, f'pop_{self.n}_seed{self.seed}.json')
        self.run_regression(filename, test_prefix, self.configDir, pars=pars)

    def run_regression(self, filename, test_prefix, config_dir, pars=None):
        pop = self.pop if pars is None else sp.Pop(**pars)
        pars = self.pars if pars is None else pars
        sc.savejson(os.path.join(config_dir, f"{test_prefix}.config.json"), pars, indent=2)

        # if default sort order is not concerned:
        popdict = dict(sorted(pop.popdict.items(), key=lambda x: x[0]))
        sc.savejson(filename, popdict, indent=2)
        self.get_pop_details(pop, self.resultdir, test_prefix)
        self.generate_reports(test_prefix=test_prefix, forced=self.generateBaseline)
        if not self.generateBaseline:
            unchanged, failed_cases = self.check_result(actual_folder=self.resultdir, test_prefix=test_prefix)
            if unchanged:
                print(f'Note: regression unchanged')
            else:
                if len(failed_cases) < 1 :
                    errormsg = self.msg_baseline_not_found(test_prefix=test_prefix,
                                                           filepattern=f"{self.n}_seed_{self.seed}")
                else:
                    print(f'Warning, regression changed! Generating report...')
                    self.generate_reports(test_prefix=test_prefix, failedcases=failed_cases)
                    errormsg = f"regression test detected changes, " \
                               f"please go to \n{self.pdfDir} " \
                               f"to review report and test results \n " \
                               f"\n\ndelete files in {os.path.join(self.expectedDir, test_prefix)} " \
                               f"and regenerate expected files using cls.generateBaseline=True\n " \
                               f"if you approve this change."
                raise ValueError(errormsg)

    def get_pop_details(self, pop, dir, title_prefix, decimal=3):
        """
        get population contact counts by age brackets and save the results as json files and plots
        Args:
            pop (pop object)    : population, either synthpops.pop.Pop, covasim.people.People, or dict
            dir (str)           : directory to save the result files
            title_prefix (str)  : used for titles in plotting
            decimal (int)       : rounding decimal places for result comparison

        Returns:
        None
        """
        os.makedirs(dir, exist_ok=True)
        # want default age brackets and to see that they line up with the average contacts by age bracket created
        age_brackets = spdd.get_census_age_brackets(**pop.loc_pars)
        age_brackets_labels = [str(age_brackets[b][0]) + '-' + str(age_brackets[b][-1]) for b in sorted(age_brackets.keys())]
        for setting_code in ['H', 'W', 'S']:
            average_contacts = self.get_average_contacts_by_brackets(pop, setting_code, decimal)
            fmt = f'%.{str(decimal)}f'
            # print(f"expected contacts by age for {code}:\n", average_contacts)
            prefix = f"{self.n}_seed_{self.seed}_{setting_code}_average_contacts"
            kwargs = sc.objdict(binned=True,
                                fontsize=10,
                                names=age_brackets_labels,
                                figdir=self.figDir,
                                figname=f"{prefix}_graph",
                                title_prefix=prefix,
                                expect_label='Expected' if self.generateBaseline else 'Test',
                                do_save=self.do_save,
                                save_dpi=100,
                                rotation=50)
            sp.plot_array(average_contacts, **kwargs)
            sc.savejson(os.path.join(dir, f"{self.n}_seed_{self.seed}_{setting_code}_average_contact.json"),
                        dict(enumerate(average_contacts)), indent=2)

            for method in ['density', 'frequency']:
                matrix = sp.calculate_contact_matrix(pop.popdict, method, setting_code)
                ageindex = spb.get_age_by_brackets_dic(age_brackets)
                agg_matrix = spb.get_aggregate_matrix(matrix, ageindex)
                textfile = os.path.join(dir, f"{self.n}_seed_{self.seed}_{setting_code}_{method}_contact_matrix.csv")
                np.savetxt(textfile, agg_matrix, delimiter=",", fmt=fmt)
                fig = sp.plot_contacts(pop.popdict, setting_code=setting_code, density_or_frequency=method, do_show=False)
                fig.savefig(os.path.join(self.figDir, f"{self.n}_seed_{self.seed}_{setting_code}_{method}_contact_matrix.png"))

    def get_average_contacts_by_brackets(self, pop, setting_code, decimal=3):
        age_brackets = sp.get_census_age_brackets(**pop.loc_pars)
        average_contacts = []
        for k in age_brackets:
            degree_df = sp.count_layer_degree(pop, layer=setting_code, ages=age_brackets[k])
            people_in_ages = sp.filter_people(pop, ages=age_brackets[k])
            average_contacts.append(0 if len(degree_df) == 0 else np.round(degree_df.sum(axis=0)['degree']/len(people_in_ages), decimals=decimal))
        return average_contacts

    def check_figs_diff(self, expected_figname, actual_figname, color=[0,0,255]):
        """
        compare expected and actual images and mark the differences to the color specified
        Args:
            expected_figname (str): expected image file
            actual_figname (str): actual image file
            color (list): [Blue, Green, Red] format to specify a color

        Returns:
            diff image filename (str)
            a diff image file will be saved as actual_filename_diff in the folder which actual_figname resides
        """
        expected_fig = cv2.imread(expected_figname)
        actual_fig = cv2.imread(actual_figname)
        # Calculate difference between two images
        diff = cv2.subtract(expected_fig, actual_fig)
        expected_copy = expected_fig.copy()
        # threshold the diff image
        # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html
        # https://en.wikipedia.org/wiki/Otsu%27s_method
        diff_gray= cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
        ret, mask = cv2.threshold(diff_gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)
        # apply the mask to the expected images to make the differences obvious (red)
        # note that color in opencv is represented as [blue, green, red]
        expected_copy[mask != 255] = [0, 0, 255]
        diff_file = f"{os.path.splitext(actual_figname)[0]}_diff{os.path.splitext(actual_figname)[1]}"
        cv2.imwrite(diff_file, expected_copy)
        return diff_file

    def check_files_diff(self, expected_file, actual_file):
        """
        check if two files' contents are identical and show diff
        Args:
            expected_file (str): expected text file
            actual_file (str): actual text file

        Returns:
        None, will raise error if files' contents are not identical
        """
        compare_result = filecmp.cmp(expected_file, actual_file, shallow=False)
        print(f"expected file: {expected_file}")
        print(f"actual file: {actual_file}")
        if not compare_result:
            with open(expected_file) as ef, open(actual_file) as af:
                eflines = ef.readlines()
                aflines = af.readlines()
                d = difflib.Differ()
                diffs = [x for x in d.compare(eflines, aflines) if x[0] in ('+', '-')]
                if diffs:
                    print("differences found:")
                    for x in diffs:
                        print(x)
                else:
                    print('No changes detected')
        else:
            print("file check completed.")
        if not compare_result:
            raise ValueError("files diff! please review and regenerate the baseline file.")

    def msg_baseline_not_found(self, test_prefix, filepattern):
        msg = f"Unable to verify results.\n Please make sure baseline files exists in: " \
            f"{os.path.join(self.expectedDir, test_prefix)} and filename prefix is {filepattern}.\n " \
            f"If baseline files do not exist, please regenerate files using cls.generateBaseline=True\n" \
            f"and copy them to {os.path.join(self.expectedDir, test_prefix)}"
        return msg

    def check_result(self, actual_folder, expected_folder=None, test_prefix="test", decimal=3, filepattern=None):
        """
        Compare all csv, json files between actual/expected folder
        files must be under "test_prefix" subfolder and have the same name in both folders
        Args:
            actual_folder (str)     : folder which results files reside
            expected_folder (str)   : folder which expected files reside
            test_prefix(str)        : test prefix to identify the test case
            decimal(int)            : rounding decimal places for result comparison
            filepattern (str)       : regex pattern to search for expected files

        Returns:
        tuple (result (bool), failed testcases (list))
        """
        passed = True
        checked = False
        failed_cases = []
        filepattern = rf'{self.n}_seed_{self.seed}*\.*' if filepattern is None else filepattern
        if not os.path.exists(actual_folder):
            raise FileNotFoundError(actual_folder)
        if expected_folder is None:
            expected_folder = os.path.join(self.expectedDir, test_prefix)
        if not os.path.exists(expected_folder):
            raise UserWarning(f"{expected_folder} does not exist, use cls.generateBaseline = True to generate them")
            return false, failed_case
        expected_files = [f for f in os.listdir(expected_folder) if re.match(filepattern, f)]
        for f in expected_files:
            print(f"\n{f}")
            if f.endswith(".csv"):
                checked = True
                expected_data = np.loadtxt(os.path.join(expected_folder, f), delimiter=",")
                actual_data = np.loadtxt(os.path.join(actual_folder, f), delimiter=",")
                if (np.round(expected_data, decimal) == np.round(actual_data, decimal)).all():
                    print("values unchanged, passed")
                else:
                    passed = False
                    failed_cases.append(os.path.basename(f).replace(".csv", "*"))
                    print("result has been changed in these indexes:\n", np.where(expected_data != actual_data)[0])
            elif f.endswith(".json"):
                expected_data = sc.loadjson(os.path.join(expected_folder, f))
                actual_data = sc.loadjson(os.path.join(actual_folder, f))
                if (expected_data == actual_data):
                    print("values unchanged, passed")
                else:
                    passed = False
                    failed_cases.append(os.path.basename(f).replace(".json", "*"))
                    diff = set(expected_data.items()).symmetric_difference(actual_data.items())
                    print("result has been changed in:\n", diff)
            else:
                print("ignored.\n")
        return passed & checked, failed_cases

    def generate_reports(self, test_prefix="", failedcases=[], forced=False):
        """
        generate pdf reports with expected /actual figs and their differences for specified test cases
        Args:
            test_prefix (str): test prefix to identify the testcase
            failedcases (list): list of failed cases, used to search for related figs by the testcase name
            forced (bool): if specified, force the report generation

        Returns:
            None. A pdf file will be generated at pdfDir specified in the test class
        """
        configs = [f for f in os.listdir(self.configDir) if os.path.isfile(os.path.join(self.configDir, f)) and f.endswith("config.json")]
        for c in configs:
            pdf = FPDF()
            pdf.add_page()
            pdf.set_font("Arial", size=12)
            name = os.path.splitext(c)[0]
            contents = ""
            # pdf.cell(w=200, h=10, txt=name, align="C")
            with open(os.path.join(self.configDir, c)) as cf:
                contents = "\n".join([line.strip() for line in cf])
            print(contents)
            pdf.multi_cell(w=100, h=5, txt=contents)
            figs = [f for f in os.listdir(self.figDir) if f.endswith("png")]
            print(f"failed cases:{failedcases}")
            if forced:
                print("force generating reports for all images and diffs")
                failedcases = figs
            for ff in figs:
                print(f"checking:{ff}")
                for fc in failedcases:
                    if fnmatch.fnmatch(ff, fc):
                        print(f"matching {fc} -> {ff}")
                        pdf.add_page()
                        actual_fig = os.path.join(self.figDir, ff)
                        pdf.image(actual_fig, w=100, h=100)
                        expected_fig = os.path.join(self.expectedDir, test_prefix, "figs", ff)
                        if os.path.exists(expected_fig):
                            diff_file = self.check_figs_diff(expected_fig, actual_fig)
                            pdf.image(expected_fig, w=100, h=100)
                            pdf.image(diff_file, w=100, h=100)
                        break
            pdf.output(os.path.join(self.pdfDir, f"{name}.pdf"))
            print("report generated:", os.path.join(self.pdfDir, f"{name}.pdf"))

    def check_similarity(self, actual, expected):
        """
        Compare two population dictionaries using contact matrix
        Assuming the canberra distance should be small
        """
        passed = True
        checked = False
        for code in ['H', 'W', 'S']:
            # check average contacts per age, if difference is small enough to tolerate
            expected_contacts = self.get_average_contacts_by_brackets(expected,code)
            print("expected contacts by age:\n", expected_contacts)
            np.savetxt(os.path.join(self.pdfDir,f"pop{self.n}_seed_{self.seed}_{code}_average_contact.csv"), expected_contacts, delimiter=",")
            actual_contacts = self.get_average_contacts_by_brackets(actual,code)
            print("actual contacts by age:\n", actual_contacts)
            max_difference = np.abs(expected_contacts-actual_contacts).max()
            checked = True
            print(f"max_difference for contacts {code} in each age bracket:{max_difference}\n")
            if max_difference > 1:
                for option in ['density', 'frequency']:
                    print(f"\ncheck:{code} with {option}")
                    actual_matrix = sp.calculate_contact_matrix(actual, density_or_frequency=option, setting_code=code)
                    # expected_matrix = sp.calculate_contact_matrix(expected, density_or_frequency=option, setting_code=code)
                    expected_matrix = np.loadtxt(os.path.join(self.expectedDir,f"pop{self.n}_seed_{self.seed}_{code}_{option}_contact_matrix.csv"), unpack=True, delimiter=",")
                    np.savetxt(os.path.join(self.pdfDir,f"pop{self.n}_seed_{self.seed}_{code}_{option}_contact_matrix.csv"), expected_matrix, delimiter=",")
                    # calculate Canberra distance
                    # assuming they should round to 0
                    d = np.abs(expected_matrix - actual_matrix).mean()
                    # d = distance.canberra(actual_matrix.flatten(), expected_matrix.flatten())
                    print(f"mean absolute difference between actual/expected contact matrix for {code}/{option} is {str(round(d, 3))}")
                    if d > 1:
                        passed = passed & False
        return checked & passed

    def cast_uid_toINT(self, dict):
        return {int(key): val for key, val in dict.items()}


# Run unit tests if called as a script
if __name__ == '__main__':
    unittest.main()
